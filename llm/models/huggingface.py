"""
Int√©gration avec Hugging Face pour utiliser LLaMA 3 ou DeepSeek R1

Pourquoi : Hugging Face permet d'utiliser des mod√®les open-source gratuitement
Comment : Utilise l'API Transformers pour charger et ex√©cuter les mod√®les OU l'API Inference
"""
from typing import Optional
import os
from pathlib import Path


class HuggingFaceLLM:
    """
    Wrapper pour utiliser les mod√®les Hugging Face (LLaMA 3, DeepSeek R1)
    
    Pourquoi : Permettre d'utiliser des LLMs open-source sans co√ªt
    Comment : Charge le mod√®le et g√©n√®re des r√©ponses √† partir des prompts
    """
    
    def __init__(self, model_name: str = "meta-llama/Meta-Llama-3-8B-Instruct"):
        """
        Initialise le mod√®le Hugging Face
        
        Args:
            model_name: Nom du mod√®le sur Hugging Face
                - "meta-llama/Meta-Llama-3-8B-Instruct" (LLaMA 3)
                - "deepseek-ai/DeepSeek-R1" (DeepSeek R1)
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """
        Charge le mod√®le Hugging Face
        
        Pourquoi : Charger le mod√®le une seule fois pour optimiser les performances
        Comment : Utilise transformers pour charger le mod√®le
        """
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            print(f"üîÑ Chargement du mod√®le {self.model_name}...")
            
            # Charger le tokenizer et le mod√®le
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else "cpu"
            )
            
            print("‚úÖ Mod√®le charg√© avec succ√®s!")
            
        except ImportError:
            print("‚ö†Ô∏è  transformers non install√©. Installer avec: pip install transformers torch")
            raise
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            print("üí° Alternative: Utiliser OpenAI ou l'API Hugging Face Inference")
            raise
    
    def generate(self, prompt: str, max_length: int = 2000, temperature: float = 0.7) -> str:
        """
        G√©n√®re une r√©ponse √† partir d'un prompt
        
        Pourquoi : G√©n√©rer les politiques de s√©curit√© √† partir des prompts structur√©s
        Comment : Utilise le mod√®le pour g√©n√©rer du texte conditionn√© par le prompt
        
        Args:
            prompt: Prompt d'entr√©e pour le LLM
            max_length: Longueur maximale de la r√©ponse
            temperature: Contr√¥le la cr√©ativit√© (0.0 = d√©terministe, 1.0 = cr√©atif)
            
        Returns:
            Texte g√©n√©r√© par le LLM
        """
        if self.model is None or self.tokenizer is None:
            self.load_model()
        
        try:
            # Pr√©parer le prompt
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            # G√©n√©rer
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # D√©coder la r√©ponse
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Retirer le prompt de la r√©ponse
            if prompt in response:
                response = response.replace(prompt, "").strip()
            
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
            return f"Erreur: {str(e)}"


class HuggingFaceAPI:
    """
    Alternative : Utiliser l'API Hugging Face Inference (plus simple, recommand√©)
    
    Pourquoi : √âviter de t√©l√©charger les gros mod√®les localement (n√©cessite GPU)
    Comment : Utilise l'API HTTP de Hugging Face
    
    Note : Cette m√©thode est recommand√©e car elle ne n√©cessite pas de GPU local
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Initialise l'API Hugging Face
        
        Args:
            api_key: Cl√© API Hugging Face (optionnel pour certains mod√®les publics)
        """
        # Charger depuis .env si disponible
        self._load_env_file()
        
        self.api_key = api_key or os.getenv("HUGGINGFACE_API_KEY")
        self.api_url = "https://api-inference.huggingface.co/models"
        
        if not self.api_key:
            print("‚ö†Ô∏è  HUGGINGFACE_API_KEY non d√©finie.")
            print("üí° Certains mod√®les n√©cessitent une cl√© API. Cr√©er un compte sur huggingface.co")
    
    def _load_env_file(self):
        """
        Charge les variables d'environnement depuis .env
        
        Pourquoi : Permettre de stocker la cl√© API dans un fichier .env local
        Comment : Utilise python-dotenv si disponible, sinon lit le fichier manuellement
        """
        env_paths = [
            Path(__file__).parent.parent / ".env",
            Path(__file__).parent.parent.parent / ".env",
        ]
        
        for env_path in env_paths:
            if env_path.exists():
                # Essayer avec python-dotenv d'abord
                try:
                    from dotenv import load_dotenv
                    load_dotenv(env_path)
                    break
                except ImportError:
                    # Fallback : lire le fichier manuellement
                    self._load_env_manual(env_path)
                    break
    
    def _load_env_manual(self, env_path: Path):
        """
        Charge les variables depuis .env manuellement (si dotenv n'est pas disponible)
        """
        try:
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    if '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip().strip('"').strip("'")
                        
                        if key and value and key not in os.environ:
                            os.environ[key] = value
        except Exception as e:
            pass  # Ignorer les erreurs silencieusement
        
    def generate(self, prompt: str, model: str = "meta-llama/Meta-Llama-3-8B-Instruct", 
                 max_length: int = 2000, temperature: float = 0.7) -> str:
        """
        G√©n√®re une r√©ponse via l'API Hugging Face
        
        Pourquoi : Utiliser les LLMs sans t√©l√©charger les mod√®les localement
        Comment : Appelle l'API REST de Hugging Face Inference
        
        Args:
            prompt: Prompt d'entr√©e
            model: Nom du mod√®le sur Hugging Face
                - "meta-llama/Meta-Llama-3-8B-Instruct" (n√©cessite cl√© API)
                - "gpt2" (public, pas de cl√© n√©cessaire)
                - "mistralai/Mistral-7B-Instruct-v0.2"
            max_length: Nombre maximum de tokens √† g√©n√©rer
            temperature: Contr√¥le la cr√©ativit√© (0.0-1.0)
            
        Returns:
            Texte g√©n√©r√©
        """
        try:
            import requests
            
            headers = {"Content-Type": "application/json"}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
            
            print(f"üîÑ Appel API Hugging Face ({model})...")
            
            response = requests.post(
                f"{self.api_url}/{model}",
                headers=headers,
                json={
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": max_length,
                        "temperature": temperature,
                        "return_full_text": False
                    }
                },
                timeout=180  # 3 minutes pour les gros mod√®les
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # G√©rer diff√©rents formats de r√©ponse
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get("generated_text", "")
                elif isinstance(result, dict):
                    generated_text = result.get("generated_text", str(result))
                else:
                    generated_text = str(result)
                
                print(f"‚úÖ R√©ponse re√ßue ({len(generated_text)} caract√®res)")
                return generated_text
                
            elif response.status_code == 503:
                # Mod√®le en cours de chargement
                return f"Erreur: Mod√®le {model} est en cours de chargement. Attendez 30 secondes et r√©essayez."
            elif response.status_code == 401:
                return f"Erreur d'authentification: V√©rifiez votre cl√© API Hugging Face"
            else:
                return f"Erreur API ({response.status_code}): {response.text[:200]}"
                
        except ImportError:
            return "Erreur: requests non install√©. Installer avec: pip install requests"
        except requests.exceptions.Timeout:
            return "Erreur: Timeout - Le mod√®le prend trop de temps √† r√©pondre. Essayez un mod√®le plus petit."
        except Exception as e:
            return f"Erreur: {str(e)}"

