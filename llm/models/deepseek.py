"""
Int√©gration avec DeepSeek R1 API

Pourquoi : DeepSeek R1 est un mod√®le performant, open-source, et beaucoup moins cher qu'OpenAI
Comment : Utilise l'API DeepSeek (similaire √† OpenAI) pour g√©n√©rer les politiques
"""
from typing import Optional
import os
from pathlib import Path


class DeepSeekLLM:
    """
    Wrapper pour utiliser l'API DeepSeek R1
    
    Pourquoi : DeepSeek R1 offre d'excellentes performances pour un co√ªt beaucoup plus faible
    Comment : Appelle l'API DeepSeek avec les prompts structur√©s
    
    Avantages vs OpenAI :
    - ‚úÖ 40x moins cher (0.55$ vs 15$ par million tokens)
    - ‚úÖ Performance comparable
    - ‚úÖ Open-source
    - ‚úÖ API similaire √† OpenAI (facile √† int√©grer)
    """
    
    def __init__(self, api_key: Optional[str] = None, model: str = "deepseek-chat"):
        """
        Initialise l'API DeepSeek
        
        Args:
            api_key: Cl√© API DeepSeek (peut √™tre dans .env ou variable d'environnement)
            model: Mod√®le √† utiliser
                - "deepseek-chat" : Mod√®le conversationnel standard
                - "deepseek-reasoner" : Mod√®le avec raisonnement (R1, plus puissant)
        """
        # Charger depuis le fichier .env si disponible
        self._load_env_file()
        
        # Priorit√© : param√®tre > variable d'environnement
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.model = model
        
        # URL de l'API DeepSeek
        self.api_base = "https://api.deepseek.com/v1"
        
        if not self.api_key:
            print("‚ö†Ô∏è  DEEPSEEK_API_KEY non d√©finie.")
            print("üí° Options:")
            print("   1. Cr√©er un compte sur https://platform.deepseek.com")
            print("   2. Cr√©er un fichier .env dans llm/ avec: DEEPSEEK_API_KEY=votre_cle")
            print("   3. D√©finir la variable d'environnement: export DEEPSEEK_API_KEY=votre_cle")
            print("   4. Passer api_key directement au constructeur")
    
    def _load_env_file(self):
        """
        Charge les variables d'environnement depuis un fichier .env
        
        Pourquoi : Permettre de stocker la cl√© API dans un fichier .env local
        Comment : Utilise python-dotenv si disponible, sinon lit le fichier manuellement
        """
        # Chercher .env dans le dossier llm/ puis √† la racine
        env_paths = [
            Path(__file__).parent.parent / ".env",  # llm/.env
            Path(__file__).parent.parent.parent / ".env",  # racine/.env
        ]
        
        for env_path in env_paths:
            if env_path.exists():
                # Essayer avec python-dotenv d'abord (m√©thode recommand√©e)
                try:
                    from dotenv import load_dotenv
                    load_dotenv(env_path)
                    print(f"‚úÖ Fichier .env charg√©: {env_path}")
                    break
                except ImportError:
                    # Fallback : lire le fichier manuellement
                    self._load_env_manual(env_path)
                    break
    
    def _load_env_manual(self, env_path: Path):
        """
        Charge les variables depuis .env manuellement (si dotenv n'est pas disponible)
        
        Pourquoi : Permettre de fonctionner m√™me sans python-dotenv install√©
        Comment : Parse le fichier ligne par ligne
        """
        try:
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # Ignorer les commentaires et lignes vides
                    if not line or line.startswith('#'):
                        continue
                    
                    # Format: KEY=VALUE
                    if '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip().strip('"').strip("'")
                        
                        # Ne pas √©craser les variables d√©j√† d√©finies
                        if key and value and key not in os.environ:
                            os.environ[key] = value
            
            print(f"‚úÖ Fichier .env charg√© manuellement: {env_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors du chargement manuel de .env: {e}")
    
    def generate(self, prompt: str, max_tokens: int = 2000, temperature: float = 0.7) -> str:
        """
        G√©n√®re une r√©ponse √† partir d'un prompt avec DeepSeek
        
        Pourquoi : G√©n√©rer les politiques de s√©curit√© avec DeepSeek R1
        Comment : Appelle l'API DeepSeek ChatCompletion (compatible OpenAI)
        
        Args:
            prompt: Prompt d'entr√©e pour le LLM
            max_tokens: Nombre maximum de tokens √† g√©n√©rer
            temperature: Contr√¥le la cr√©ativit√© (0.0-1.0)
                Note : DeepSeek R1 recommande temperature=0 pour le raisonnement
            
        Returns:
            Texte g√©n√©r√© par DeepSeek
        """
        if not self.api_key:
            return "Erreur: DEEPSEEK_API_KEY non configur√©e. D√©finir dans .env ou variable d'environnement"
        
        try:
            # DeepSeek utilise une API compatible OpenAI
            from openai import OpenAI
            
            # Utiliser l'API DeepSeek avec le client OpenAI (compatible)
            client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            
            print(f"üîÑ Appel API DeepSeek ({self.model})...")
            
            # Pour DeepSeek R1 (reasoner), utiliser temperature=0 pour le raisonnement
            if "reasoner" in self.model.lower():
                temperature = 0.0
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un expert en cybers√©curit√© et conformit√©. Tu g√©n√®res des politiques de s√©curit√© professionnelles et conformes aux standards internationaux (NIST CSF, ISO 27001). R√©ponds toujours en fran√ßais."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=180  # Timeout de 3 minutes (DeepSeek peut prendre plus de temps pour le raisonnement)
            )
            
            generated_text = response.choices[0].message.content.strip()
            print(f"‚úÖ R√©ponse re√ßue ({len(generated_text)} caract√®res)")
            
            return generated_text
            
        except ImportError:
            return "Erreur: openai non install√©. Installer avec: pip install openai"
        except Exception as e:
            error_msg = str(e)
            if "api_key" in error_msg.lower() or "authentication" in error_msg.lower():
                return f"Erreur d'authentification: V√©rifiez votre cl√© API DeepSeek. {error_msg}"
            elif "rate limit" in error_msg.lower():
                return f"Erreur: Limite de taux atteinte. Attendez un moment avant de r√©essayer. {error_msg}"
            elif "model" in error_msg.lower():
                return f"Erreur: Mod√®le {self.model} non disponible. V√©rifiez le nom du mod√®le. {error_msg}"
            else:
                return f"Erreur DeepSeek: {error_msg}"

