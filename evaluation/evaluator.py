"""
√âvaluateur Principal pour Comparer les Politiques G√©n√©r√©es

Pourquoi : Comparer les politiques g√©n√©r√©es par diff√©rents LLMs (DeepSeek R1 vs LLaMA 3)
Comment : Charge les politiques, les compare avec des r√©f√©rences, g√©n√®re un rapport
"""
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime

from .bleu_rouge import evaluate_policy


class PolicyEvaluator:
    """
    √âvaluateur pour comparer les politiques g√©n√©r√©es par diff√©rents LLMs
    
    Pourquoi : Comparer la qualit√© des politiques g√©n√©r√©es par DeepSeek R1 vs LLaMA 3
    Comment : Utilise BLEU et ROUGE-L pour comparer avec des r√©f√©rences
    """
    
    def __init__(self, reference_dir: str = "evaluation/reference_policies"):
        """
        Initialise l'√©valuateur
        
        Args:
            reference_dir: Dossier contenant les politiques de r√©f√©rence
        """
        self.reference_dir = Path(reference_dir)
        self.references: Dict[str, List[str]] = {}
        
    def load_references(self):
        """
        Charge toutes les politiques de r√©f√©rence depuis les fichiers
        
        Pourquoi : Avoir des r√©f√©rences pour comparer les politiques g√©n√©r√©es
        Comment : Lit tous les fichiers .txt dans reference_policies/
        """
        print("üìö Chargement des politiques de r√©f√©rence...")
        
        # Charger r√©f√©rences NIST CSF
        nist_dir = self.reference_dir / "nist_csf"
        if nist_dir.exists():
            nist_refs = []
            for ref_file in nist_dir.glob("*.txt"):
                with open(ref_file, 'r', encoding='utf-8') as f:
                    nist_refs.append(f.read())
            if nist_refs:
                self.references['nist_csf'] = nist_refs
                print(f"‚úÖ {len(nist_refs)} r√©f√©rences NIST CSF charg√©es")
        
        # Charger r√©f√©rences ISO 27001
        iso_dir = self.reference_dir / "iso27001"
        if iso_dir.exists():
            iso_refs = []
            for ref_file in iso_dir.glob("*.txt"):
                with open(ref_file, 'r', encoding='utf-8') as f:
                    iso_refs.append(f.read())
            if iso_refs:
                self.references['iso27001'] = iso_refs
                print(f"‚úÖ {len(iso_refs)} r√©f√©rences ISO 27001 charg√©es")
        
        if not self.references:
            print("‚ö†Ô∏è  Aucune r√©f√©rence trouv√©e. Cr√©ez des fichiers .txt dans evaluation/reference_policies/")
    
    def load_generated_policies(self, policies_dir: str) -> Dict[str, Dict[str, str]]:
        """
        Charge les politiques g√©n√©r√©es par les LLMs
        
        Pourquoi : Comparer les politiques g√©n√©r√©es par diff√©rents mod√®les
        Comment : Lit les fichiers dans llm/policies/ organis√©s par mod√®le
        
        Args:
            policies_dir: Dossier contenant les politiques g√©n√©r√©es
            
        Returns:
            Dictionnaire organis√© par mod√®le LLM et type de politique
            {
                'deepseek': {
                    'nist_csf/PROTECT': 'texte politique...',
                    'iso27001/A.9.2.1': 'texte politique...'
                },
                'llama3': {
                    ...
                }
            }
        """
        policies = {}
        policies_path = Path(policies_dir)
        
        if not policies_path.exists():
            print(f"‚ö†Ô∏è  Dossier non trouv√©: {policies_dir}")
            return policies
        
        print(f"üìÇ Chargement des politiques g√©n√©r√©es depuis {policies_dir}...")
        
        # Chercher les politiques par mod√®le
        for model_dir in policies_path.iterdir():
            if not model_dir.is_dir():
                continue
            
            model_name = model_dir.name.lower()
            if model_name not in ['deepseek', 'llama3', 'huggingface']:
                continue
            
            policies[model_name] = {}
            
            # Chercher les politiques NIST CSF
            nist_dir = model_dir / "nist_csf"
            if nist_dir.exists():
                for policy_file in nist_dir.glob("*.txt"):
                    category = policy_file.stem
                    with open(policy_file, 'r', encoding='utf-8') as f:
                        policies[model_name][f"nist_csf/{category}"] = f.read()
            
            # Chercher les politiques ISO 27001
            iso_dir = model_dir / "iso27001"
            if iso_dir.exists():
                for policy_file in iso_dir.glob("*.txt"):
                    control = policy_file.stem
                    with open(policy_file, 'r', encoding='utf-8') as f:
                        policies[model_name][f"iso27001/{control}"] = f.read()
        
        print(f"‚úÖ {sum(len(ps) for ps in policies.values())} politiques charg√©es")
        return policies
    
    def evaluate_all_policies(self, generated_policies: Dict[str, Dict[str, str]]) -> Dict:
        """
        √âvalue toutes les politiques g√©n√©r√©es
        
        Pourquoi : Obtenir des scores pour toutes les politiques et comparer les mod√®les
        Comment : Compare chaque politique avec les r√©f√©rences appropri√©es
        
        Args:
            generated_policies: Dictionnaire des politiques g√©n√©r√©es par mod√®le
            
        Returns:
            Dictionnaire des r√©sultats d'√©valuation
        """
        results = {}
        
        print("\nüîç √âvaluation des politiques...")
        
        for model_name, policies in generated_policies.items():
            print(f"\nüìä √âvaluation pour {model_name.upper()}...")
            results[model_name] = {}
            
            for policy_key, policy_text in policies.items():
                # D√©terminer le type de r√©f√©rence √† utiliser
                if policy_key.startswith("nist_csf/"):
                    ref_type = 'nist_csf'
                    ref_key = policy_key.replace("nist_csf/", "")
                elif policy_key.startswith("iso27001/"):
                    ref_type = 'iso27001'
                    ref_key = policy_key.replace("iso27001/", "")
                else:
                    continue
                
                # Obtenir les r√©f√©rences
                references = self.references.get(ref_type, [])
                
                if not references:
                    print(f"‚ö†Ô∏è  Pas de r√©f√©rence pour {policy_key}")
                    continue
                
                # √âvaluer
                scores = evaluate_policy(policy_text, references)
                results[model_name][policy_key] = scores
                
                print(f"  ‚úÖ {policy_key}: BLEU={scores['bleu']:.3f}, ROUGE-L F={scores['rouge_l_f']:.3f}")
        
        return results
    
    def calculate_model_averages(self, results: Dict) -> Dict[str, Dict[str, float]]:
        """
        Calcule les moyennes pour chaque mod√®le
        
        Pourquoi : Comparer les mod√®les avec des statistiques agr√©g√©es
        Comment : Moyenne de tous les scores pour chaque mod√®le
        
        Args:
            results: Dictionnaire des r√©sultats d'√©valuation
            
        Returns:
            Dictionnaire des moyennes par mod√®le
        """
        averages = {}
        
        for model_name, model_results in results.items():
            if not model_results:
                continue
            
            total_policies = len(model_results)
            avg_bleu = sum(r['bleu'] for r in model_results.values()) / total_policies
            avg_rouge_f = sum(r['rouge_l_f'] for r in model_results.values()) / total_policies
            avg_rouge_p = sum(r['rouge_l_p'] for r in model_results.values()) / total_policies
            avg_rouge_r = sum(r['rouge_l_r'] for r in model_results.values()) / total_policies
            
            averages[model_name] = {
                'avg_bleu': round(avg_bleu, 4),
                'avg_rouge_l_f': round(avg_rouge_f, 4),
                'avg_rouge_l_p': round(avg_rouge_p, 4),
                'avg_rouge_l_r': round(avg_rouge_r, 4),
                'num_policies': total_policies
            }
        
        return averages
    
    def generate_report(self, results: Dict, averages: Dict, output_file: str = "evaluation/evaluation_report.json"):
        """
        G√©n√®re un rapport d'√©valuation complet
        
        Pourquoi : Documenter les r√©sultats de l'√©valuation pour comparaison
        Comment : Sauvegarde un rapport JSON avec tous les scores
        
        Args:
            results: R√©sultats d√©taill√©s par politique
            averages: Moyennes par mod√®le
            output_file: Fichier de sortie
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': averages,
            'detailed_results': results
        }
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Rapport sauvegard√©: {output_file}")
        return report
    
    def print_comparison(self, averages: Dict):
        """
        Affiche une comparaison claire des mod√®les
        
        Pourquoi : Visualiser rapidement quel mod√®le performe le mieux
        Comment : Tableau format√© avec les scores moyens
        
        Args:
            averages: Moyennes par mod√®le
        """
        print("\n" + "="*70)
        print("üìä COMPARAISON DES MOD√àLES LLM")
        print("="*70)
        
        if not averages:
            print("‚ö†Ô∏è  Aucun r√©sultat √† comparer")
            return
        
        # En-t√™tes
        print(f"\n{'Mod√®le':<20} {'BLEU':<10} {'ROUGE-L F':<12} {'ROUGE-L P':<12} {'ROUGE-L R':<12} {'Politiques':<10}")
        print("-" * 70)
        
        # Tri par BLEU score d√©croissant
        sorted_models = sorted(
            averages.items(),
            key=lambda x: x[1]['avg_bleu'],
            reverse=True
        )
        
        for model_name, scores in sorted_models:
            print(f"{model_name.upper():<20} "
                  f"{scores['avg_bleu']:<10.4f} "
                  f"{scores['avg_rouge_l_f']:<12.4f} "
                  f"{scores['avg_rouge_l_p']:<12.4f} "
                  f"{scores['avg_rouge_l_r']:<12.4f} "
                  f"{scores['num_policies']:<10}")
        
        print("\n" + "="*70)
        
        # D√©terminer le meilleur mod√®le
        best_model = max(averages.items(), key=lambda x: x[1]['avg_bleu'])
        print(f"\nüèÜ MEILLEUR MOD√àLE: {best_model[0].upper()}")
        print(f"   BLEU Score: {best_model[1]['avg_bleu']:.4f}")
        print(f"   ROUGE-L F-Score: {best_model[1]['avg_rouge_l_f']:.4f}")
        print("="*70 + "\n")


def main():
    """
    Fonction principale pour ex√©cuter l'√©valuation compl√®te
    
    Pourquoi : Permettre d'ex√©cuter l'√©valuateur depuis la ligne de commande
    Comment : Charge r√©f√©rences, politiques g√©n√©r√©es, √©value et g√©n√®re rapport
    """
    evaluator = PolicyEvaluator()
    
    # Charger les r√©f√©rences
    evaluator.load_references()
    
    if not evaluator.references:
        print("‚ùå Erreur: Aucune r√©f√©rence charg√©e. Cr√©ez des politiques de r√©f√©rence d'abord.")
        return
    
    # Charger les politiques g√©n√©r√©es
    generated_policies = evaluator.load_generated_policies("llm/policies")
    
    if not generated_policies:
        print("‚ùå Erreur: Aucune politique g√©n√©r√©e trouv√©e. G√©n√©rez des politiques d'abord.")
        return
    
    # √âvaluer toutes les politiques
    results = evaluator.evaluate_all_policies(generated_policies)
    
    # Calculer les moyennes
    averages = evaluator.calculate_model_averages(results)
    
    # Afficher la comparaison
    evaluator.print_comparison(averages)
    
    # G√©n√©rer le rapport
    evaluator.generate_report(results, averages)
    
    print("\n‚úÖ √âvaluation termin√©e !")


if __name__ == "__main__":
    main()

