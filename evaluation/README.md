# ğŸ“Š Module d'Ã‰valuation - BLEU & ROUGE-L

## ğŸ¯ Objectif

Ã‰valuer la qualitÃ© des politiques gÃ©nÃ©rÃ©es par les LLMs en les comparant avec des politiques de rÃ©fÃ©rence utilisant les mÃ©triques **BLEU** et **ROUGE-L**.

## ğŸ” MÃ©triques UtilisÃ©es

### BLEU Score
- **Quoi** : Mesure la similaritÃ© n-gram entre le texte gÃ©nÃ©rÃ© et les rÃ©fÃ©rences
- **Pourquoi** : Indique si le texte gÃ©nÃ©rÃ© utilise un vocabulaire et des phrases similaires aux rÃ©fÃ©rences
- **Gamme** : 0 (complÃ¨tement diffÃ©rent) Ã  1 (identique)

### ROUGE-L Score
- **Quoi** : Mesure le recouvrement basÃ© sur la plus longue sous-sÃ©quence commune (LCS)
- **Pourquoi** : Indique si les idÃ©es principales sont prÃ©sentes dans le texte gÃ©nÃ©rÃ©
- **Composantes** :
  - **F-Score** : Moyenne harmonique de prÃ©cision et rappel
  - **Precision** : Combien du texte gÃ©nÃ©rÃ© est pertinent
  - **Recall** : Combien des rÃ©fÃ©rences est couvert

## ğŸ“ Structure

```
evaluation/
â”œâ”€â”€ bleu_rouge.py              # ImplÃ©mentation BLEU et ROUGE-L
â”œâ”€â”€ evaluator.py               # Ã‰valuateur principal
â”œâ”€â”€ reference_policies/        # Politiques de rÃ©fÃ©rence
â”‚   â”œâ”€â”€ nist_csf/             # RÃ©fÃ©rences NIST CSF
â”‚   â””â”€â”€ iso27001/             # RÃ©fÃ©rences ISO 27001
â”œâ”€â”€ evaluation_report.json     # Rapport gÃ©nÃ©rÃ©
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸš€ Utilisation

### 1. PrÃ©parer les Politiques de RÃ©fÃ©rence

CrÃ©ez des fichiers `.txt` dans `evaluation/reference_policies/` :

**NIST CSF** :
```
evaluation/reference_policies/nist_csf/PROTECT_1.txt
evaluation/reference_policies/nist_csf/IDENTIFY_1.txt
```

**ISO 27001** :
```
evaluation/reference_policies/iso27001/A.9.2.1.txt
evaluation/reference_policies/iso27001/A.12.6.1.txt
```

### 2. Organiser les Politiques GÃ©nÃ©rÃ©es

Les politiques gÃ©nÃ©rÃ©es doivent Ãªtre organisÃ©es par modÃ¨le LLM :

```
llm/policies/
â”œâ”€â”€ deepseek/
â”‚   â”œâ”€â”€ nist_csf/
â”‚   â”‚   â””â”€â”€ PROTECT.txt
â”‚   â””â”€â”€ iso27001/
â”‚       â””â”€â”€ A.9.2.1.txt
â””â”€â”€ llama3/
    â”œâ”€â”€ nist_csf/
    â”‚   â””â”€â”€ PROTECT.txt
    â””â”€â”€ iso27001/
        â””â”€â”€ A.9.2.1.txt
```

### 3. ExÃ©cuter l'Ã‰valuation

```bash
cd mon-projet-parfumerie
python3 evaluation/evaluator.py
```

### 4. Consulter les RÃ©sultats

Le rapport est gÃ©nÃ©rÃ© dans `evaluation/evaluation_report.json` et affichÃ© dans le terminal.

## ğŸ“Š InterprÃ©tation des Scores

- **BLEU > 0.3** : Bonne similaritÃ© avec les rÃ©fÃ©rences
- **BLEU > 0.5** : TrÃ¨s bonne similaritÃ©
- **BLEU > 0.7** : Excellent (presque identique)

- **ROUGE-L F > 0.4** : Bon recouvrement des idÃ©es
- **ROUGE-L F > 0.6** : TrÃ¨s bon recouvrement
- **ROUGE-L F > 0.8** : Excellent recouvrement

## ğŸ”„ Comparaison de ModÃ¨les

L'Ã©valuateur compare automatiquement :
- **DeepSeek R1** vs **LLaMA 3**
- DÃ©termine le meilleur modÃ¨le basÃ© sur les scores moyens

## ğŸ“ Exemple de Rapport

```json
{
  "timestamp": "2024-11-02T10:30:00",
  "summary": {
    "deepseek": {
      "avg_bleu": 0.4523,
      "avg_rouge_l_f": 0.6234,
      "num_policies": 10
    },
    "llama3": {
      "avg_bleu": 0.3891,
      "avg_rouge_l_f": 0.5678,
      "num_policies": 10
    }
  }
}
```

## ğŸ’¡ Notes

- Les mÃ©triques sont implÃ©mentÃ©es en Python pur (pas de dÃ©pendances externes)
- Les politiques de rÃ©fÃ©rence doivent Ãªtre de qualitÃ© professionnelle
- Plus de rÃ©fÃ©rences = Ã©valuation plus robuste (recommandÃ©: 3-5 par catÃ©gorie)

