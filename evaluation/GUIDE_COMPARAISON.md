# üî¨ Guide de Comparaison des Mod√®les LLM

## üéØ Objectif

Comparer les performances de **DeepSeek R1** et **LLaMA 3** pour g√©n√©rer des politiques de s√©curit√© conformes NIST CSF et ISO 27001.

## üìã Processus de Comparaison

### √âtape 1 : G√©n√©ration des Politiques

G√©n√©rer les m√™mes politiques avec les deux mod√®les :

```bash
# Option 1 : Utiliser le script de comparaison automatique
python3 llm/compare_models.py parser/reports/normalized_vulnerabilities.json

# Option 2 : G√©n√©rer manuellement
# DeepSeek R1
python3 llm/policy_generator.py parser/reports/normalized_vulnerabilities.json deepseek deepseek-chat

# LLaMA 3 (Hugging Face)
python3 llm/policy_generator.py parser/reports/normalized_vulnerabilities.json huggingface meta-llama/Meta-Llama-3-8B-Instruct
```

### √âtape 2 : Organisation des Politiques

Les politiques doivent √™tre organis√©es ainsi :

```
llm/policies/
‚îú‚îÄ‚îÄ deepseek/
‚îÇ   ‚îú‚îÄ‚îÄ nist_csf/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PROTECT.txt
‚îÇ   ‚îî‚îÄ‚îÄ iso27001/
‚îÇ       ‚îî‚îÄ‚îÄ A.9.2.1.txt
‚îî‚îÄ‚îÄ llama3/
    ‚îú‚îÄ‚îÄ nist_csf/
    ‚îÇ   ‚îî‚îÄ‚îÄ PROTECT.txt
    ‚îî‚îÄ‚îÄ iso27001/
        ‚îî‚îÄ‚îÄ A.9.2.1.txt
```

### √âtape 3 : √âvaluation

Ex√©cuter l'√©valuateur :

```bash
python3 evaluation/evaluator.py
```

### √âtape 4 : Consulter les R√©sultats

Le rapport est g√©n√©r√© dans `evaluation/evaluation_report.json` et affich√© dans le terminal.

## üìä M√©triques Compar√©es

### BLEU Score
- Mesure la similarit√© n-gram avec les r√©f√©rences
- Plus √©lev√© = meilleur vocabulaire et structure

### ROUGE-L F-Score
- Mesure le recouvrement des id√©es principales
- Plus √©lev√© = meilleure couverture des concepts

### ROUGE-L Precision
- Combien du texte g√©n√©r√© est pertinent

### ROUGE-L Recall
- Combien des r√©f√©rences est couvert

## üèÜ D√©termination du Meilleur Mod√®le

L'√©valuateur compare automatiquement et d√©termine le meilleur mod√®le bas√© sur :
1. **Score BLEU moyen** (principal crit√®re)
2. **Score ROUGE-L F moyen** (crit√®re secondaire)
3. **Nombre de politiques g√©n√©r√©es** (compl√©tude)

## üìù Exemple de Sortie

```
üìä COMPARAISON DES MOD√àLES LLM
======================================================================

Mod√®le               BLEU       ROUGE-L F    ROUGE-L P    ROUGE-L R    Politiques
----------------------------------------------------------------------
DEEPSEEK             0.4523     0.6234       0.5891       0.6587       10
LLAMA3               0.3891     0.5678       0.5213       0.6156       10

======================================================================

üèÜ MEILLEUR MOD√àLE: DEEPSEEK
   BLEU Score: 0.4523
   ROUGE-L F-Score: 0.6234
======================================================================
```

## üí° Interpr√©tation

- **Diff√©rence > 0.05** : Mod√®le significativement meilleur
- **Diff√©rence < 0.05** : Mod√®les √©quivalents (choix selon autres crit√®res)
- **DeepSeek g√©n√©ralement plus rapide** : API plus performante
- **LLaMA 3 gratuit** : Mais n√©cessite permissions Hugging Face

## üîç Analyse Approfondie

Pour une analyse plus d√©taill√©e :

1. **Lire le rapport JSON** : `evaluation/evaluation_report.json`
2. **Comparer politique par politique** : Voir les scores individuels
3. **Analyser les diff√©rences qualitatives** : Lire les politiques g√©n√©r√©es
4. **Consid√©rer les co√ªts** : DeepSeek ~0.55$/1M tokens vs LLaMA gratuit

## ‚úÖ R√©sultat Final

√Ä la fin, vous aurez :
- ‚úÖ Politiques g√©n√©r√©es par les deux mod√®les
- ‚úÖ Scores d'√©valuation BLEU et ROUGE-L
- ‚úÖ D√©termination du meilleur mod√®le
- ‚úÖ Rapport JSON pour documentation

