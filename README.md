# ğŸ”’ DevSecOps - GÃ©nÃ©ration Automatique de Politiques de SÃ©curitÃ© avec IA

## ğŸ¯ Objectif

Transformer automatiquement les rapports techniques de vulnÃ©rabilitÃ©s (SAST, SCA, DAST) en **politiques de sÃ©curitÃ© conformes** aux standards internationaux (NIST CSF, ISO/IEC 27001) en utilisant des **Large Language Models (LLMs)**.

## ğŸ—ï¸ Architecture

```
Pipeline CI/CD â†’ Rapports VulnÃ©rabilitÃ©s â†’ Parser â†’ LLM â†’ Politiques Conformes â†’ Ã‰valuation
```

### Composants

1. **Pipeline CI/CD** (`.github/workflows/devsecops-pipeline.yml`)
   - SAST : SpotBugs (Java) + ESLint (JavaScript)
   - SCA : OWASP Dependency-Check + npm audit
   - DAST : OWASP ZAP

2. **Parser** (`parser/`)
   - Normalisation des rapports XML/JSON en format standardisÃ©
   - Support SAST, SCA, DAST

3. **GÃ©nÃ©ration LLM** (`llm/`)
   - DeepSeek R1 (recommandÃ©) ou LLaMA 3
   - GÃ©nÃ©ration de politiques NIST CSF et ISO 27001

4. **Ã‰valuation** (`evaluation/`)
   - MÃ©triques BLEU et ROUGE-L
   - Comparaison des modÃ¨les LLM

## ğŸš€ DÃ©marrage Rapide

### 1. Configuration

```bash
# Installer les dÃ©pendances
pip3 install --user --break-system-packages openai requests python-dotenv

# Configurer les clÃ©s API (llm/.env)
DEEPSEEK_API_KEY=sk-...
HUGGINGFACE_API_KEY=hf_...
```

### 2. ExÃ©cution

```bash
# 1. Parser les vulnÃ©rabilitÃ©s
python3 parser/main_parser.py

# 2. GÃ©nÃ©rer des politiques
python3 llm/policy_generator.py

# 3. Comparer les modÃ¨les (optionnel)
python3 llm/compare_models.py parser/reports/normalized_vulnerabilities.json

# 4. Ã‰valuer les politiques
python3 evaluation/evaluator.py
```

## ğŸ“ Structure du Projet

```
mon-projet-parfumerie/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ devsecops-pipeline.yml    # Pipeline CI/CD
â”œâ”€â”€ parser/                        # Parser de vulnÃ©rabilitÃ©s
â”‚   â”œâ”€â”€ main_parser.py
â”‚   â””â”€â”€ parsers/
â”œâ”€â”€ llm/                          # GÃ©nÃ©ration de politiques
â”‚   â”œâ”€â”€ policy_generator.py
â”‚   â”œâ”€â”€ models/                   # DeepSeek, HuggingFace
â”‚   â””â”€â”€ prompts/                   # Prompts NIST/ISO
â”œâ”€â”€ evaluation/                    # Ã‰valuation BLEU/ROUGE-L
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ reference_policies/
â””â”€â”€ backend/ & frontend/           # Application e-commerce
```

## ğŸ“š Documentation

- **Pipeline** : `DEVSECOPS.md`
- **SAST/SCA/DAST** : `GUIDE_SAST_SCA_DAST.md`
- **LLM** : `llm/README.md`, `llm/USAGE.md`, `llm/DEEPSEEK_SETUP.md`
- **Ã‰valuation** : `evaluation/README.md`, `evaluation/GUIDE_COMPARAISON.md`

## ğŸ”§ Configuration LLM

### DeepSeek R1 (RecommandÃ©)
- ModÃ¨le par dÃ©faut
- Ã‰conomique (0.55$/1M tokens)
- Configuration : `llm/DEEPSEEK_SETUP.md`

### LLaMA 3 (Hugging Face)
- Gratuit via API
- Configuration : `llm/USAGE.md`

## ğŸ“Š Ã‰valuation

Les politiques gÃ©nÃ©rÃ©es sont Ã©valuÃ©es avec :
- **BLEU Score** : SimilaritÃ© avec les rÃ©fÃ©rences
- **ROUGE-L** : Recouvrement des idÃ©es principales

Rapport gÃ©nÃ©rÃ© : `evaluation/evaluation_report.json`

## ğŸ“ Technologies

- **CI/CD** : GitHub Actions
- **SAST** : SpotBugs, ESLint
- **SCA** : OWASP Dependency-Check, npm audit
- **DAST** : OWASP ZAP
- **LLM** : DeepSeek R1, LLaMA 3 (Hugging Face)
- **Ã‰valuation** : BLEU, ROUGE-L (implÃ©mentation Python)

## ğŸ“ Licence

Ce projet est dÃ©veloppÃ© dans le cadre d'un projet acadÃ©mique DevSecOps.

## ğŸ‘¥ Auteurs

Ã‰quipe DevSecOps - IntÃ©gration de l'IA GÃ©nÃ©rative dans DevSecOps

